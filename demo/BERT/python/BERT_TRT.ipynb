{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/6/6d/Nvidia_image_logo.svg\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# QA Inference on BERT using TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "Bidirectional Embedding Representations from Transformers (BERT), is a method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. \n",
    "\n",
    "The original paper can be found here: https://arxiv.org/abs/1810.04805.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Learning objectives\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Inference on Question Answering (QA) task with BERT Base/Large model\n",
    "- The use fine-tuned NVIDIA BERT models\n",
    "- Use of BERT model with TRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Requirements\n",
    "\n",
    "Please refer to the ReadMe file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BERT Inference: Question Answering\n",
    "\n",
    "We can run inference on a fine-tuned BERT model for tasks like Question Answering.\n",
    "\n",
    "Here we use a BERT model fine-tuned on a [SQuaD 2.0 Dataset](https://rajpurkar.github.io/SQuAD-explorer/) which contains 100,000+ question-answer pairs on 500+ articles combined with over 50,000 new, unanswerable questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Paragraph and Queries\n",
    "\n",
    "The paragraph and the questions can be customized by changing the text below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_text = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of landing a man on the Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = \"What project put the first Americans into space?\"\n",
    "#question_text =  \"What year did the first manned Apollo flight occur?\"\n",
    "#question_text =  \"What President is credited with the original notion of putting Americans in space?\"\n",
    "#question_text =  \"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we ask our BERT model questions related to the following paragraph:\n",
    "\n",
    "**The Apollo Program**\n",
    "_\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of landing a man on the Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\"_\n",
    "\n",
    "The questions and relative answers expected are shown below:\n",
    "\n",
    " - **Q1:** \"What project put the first Americans into space?\" \n",
    "  - **A1:** \"Project Mercury\"\n",
    " - **Q2:** \"What program was created to carry out these projects and missions?\"\n",
    "  - **A2:** \"The Apollo program\"\n",
    " - **Q3:** \"What year did the first manned Apollo flight occur?\"\n",
    "  - **A3:** \"1968\"\n",
    " - **Q4:** \"What President is credited with the original notion of putting Americans in space?\"\n",
    "  - **A4:** \"John F. Kennedy\"\n",
    " - **Q5:** \"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\"\n",
    "  - **A5:** \"Soviet Union\"\n",
    " - **Q6:** \"How long did Project Apollo run?\"\n",
    "  - **A6:** \"1961 to 1972\"\n",
    " - **Q7:** \"What program helped develop space travel techniques that Project Apollo used?\"\n",
    "  - **A7:** \"Gemini Mission\"\n",
    " - **Q8:** \"What space station supported three manned missions in 1973-1974?\"\n",
    "  - **A8:** \"Skylab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Let's convert the paragraph and the question to BERT input with the help of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_text\n",
      "  Downloading tensorflow_text-2.3.0-cp37-cp37m-manylinux1_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow<2.4,>=2.3.0\n",
      "  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "\u001b[K     |███████████████████████████▌    | 275.5 MB 111.4 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 320.4 MB 51 kB/s \n",
      "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (3.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.1.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.34.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (2.3.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (3.12.4)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.18.5)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (0.9.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (1.30.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (2.3.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow_text) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow<2.4,>=2.3.0->tensorflow_text) (49.2.1.post20200802)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.20.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (2.10)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (1.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/tf2.3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow_text) (3.1.0)\n",
      "Installing collected packages: tensorflow, tensorflow-text\n",
      "Successfully installed tensorflow-2.3.0 tensorflow-text-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert\n",
      "  Downloading bert-2.2.0.tar.gz (3.5 kB)\n",
      "Collecting erlastic\n",
      "  Downloading erlastic-2.0.0.tar.gz (6.8 kB)\n",
      "Building wheels for collected packages: bert, erlastic\n",
      "  Building wheel for bert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bert: filename=bert-2.2.0-py3-none-any.whl size=3754 sha256=4ab9e0198851fa11d3e2b70ab0da55e86c780437f531fafb6dfba746a512a2fe\n",
      "  Stored in directory: /home/fabriziomilo/.cache/pip/wheels/bb/31/1b/c05f362e347429b7436954d1a2280fe464731e8f569123a848\n",
      "  Building wheel for erlastic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for erlastic: filename=erlastic-2.0.0-py3-none-any.whl size=6787 sha256=5ba0a79c77a1c6a619c4b8c242f670a2dc91f31682df2b1cd9294642e7d77bd4\n",
      "  Stored in directory: /home/fabriziomilo/.cache/pip/wheels/94/f1/b4/0b98b1e94775da6a0b1130e342d22af05cd269e1172c19f40f\n",
      "Successfully built bert erlastic\n",
      "Installing collected packages: erlastic, bert\n",
      "Successfully installed bert-2.2.0 erlastic-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-04 14:02:48--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12257 (12K) [text/plain]\n",
      "Saving to: ‘tokenizer.py’\n",
      "\n",
      "tokenizer.py        100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-08-04 14:02:48 (87.5 MB/s) - ‘tokenizer.py’ saved [12257/12257]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O tokenizer.py https://raw.githubusercontent.com/google-research/bert/master/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.gfile =tf.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "./data/uncased_L-12_H-768_A-12/vocab.txt; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-822a4ae202b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#tokenizer = tokenization.FullTokenizer(vocab_file=\"./data/uncased_L-24_H-1024_A-16/vocab.txt\", do_lower_case=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/uncased_L-12_H-768_A-12/vocab.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# The maximum number of tokens for the question. Questions longer than this will be truncated to this length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/content/src/TensorRT/demo/BERT/python/tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/content/src/TensorRT/demo/BERT/python/tokenizer.py\u001b[0m in \u001b[0;36mload_vocab\u001b[0;34m(vocab_file)\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m       \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf2.3/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;34mr\"\"\"Reads the next line, keeping \\n. At EOF, returns ''.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf2.3/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m                                            \"File isn't open for reading\")\n\u001b[1;32m     78\u001b[0m       self._read_buf = _pywrap_file_io.BufferedInputStream(\n\u001b[0;32m---> 79\u001b[0;31m           self.__name, 1024 * 512)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ./data/uncased_L-12_H-768_A-12/vocab.txt; No such file or directory"
     ]
    }
   ],
   "source": [
    "import data_processing as dp\n",
    "#import tokenization\n",
    "\n",
    "#Large\n",
    "#tokenizer = tokenization.FullTokenizer(vocab_file=\"./data/uncased_L-24_H-1024_A-16/vocab.txt\", do_lower_case=True)\n",
    "#Base\n",
    "tokenizer = tokenizer.FullTokenizer(vocab_file=\"./data/uncased_L-12_H-768_A-12/vocab.txt\", do_lower_case=True)\n",
    "\n",
    "# The maximum number of tokens for the question. Questions longer than this will be truncated to this length.\n",
    "max_query_length = 64\n",
    "\n",
    "# When splitting up a long document into chunks, how much stride to take between chunks.\n",
    "doc_stride = 128\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization. \n",
    "# Sequences longer than this will be truncated, and sequences shorter \n",
    "max_seq_length = 384\n",
    "\n",
    "# Extract tokecs from the paragraph\n",
    "doc_tokens = dp.convert_doc_tokens(paragraph_text)\n",
    "\n",
    "# Extract features from the paragraph and question\n",
    "features = dp.convert_examples_to_features(doc_tokens, question_text, tokenizer, max_seq_length, doc_stride, max_query_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "nvinfer =  ctypes.CDLL(\"libnvinfer_plugin.so\", mode = ctypes.RTLD_GLOBAL)\n",
    "cm = ctypes.CDLL(\"./build/libcommon.so\", mode = ctypes.RTLD_GLOBAL) \n",
    "pg = ctypes.CDLL(\"./build/libbert_plugins.so\", mode = ctypes.RTLD_GLOBAL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# For this example we are going to use batch size 1\n",
    "max_batch_size = 1\n",
    "\n",
    "# Load the Large BERT Engine\n",
    "# with open(\"./bert_python.engine\", \"rb\") as f, \\\n",
    "#    trt.Runtime(TRT_LOGGER) as runtime, \\\n",
    "#    runtime.deserialize_cuda_engine(f.read()) as engine, \\\n",
    "#    engine.create_execution_context() as context:\n",
    "\n",
    "# Load the Base BERT Engine\n",
    "with open(\"./bert_python_base.engine\", \"rb\") as f, \\\n",
    "    trt.Runtime(TRT_LOGGER) as runtime, \\\n",
    "    runtime.deserialize_cuda_engine(f.read()) as engine, \\\n",
    "    engine.create_execution_context() as context:\n",
    "\n",
    "    print(\"List engine binding:\")\n",
    "    for binding in engine:\n",
    "        print(\" - {}: {}, Shape {}, {}\".format(\n",
    "            \"Input\" if engine.binding_is_input(binding) else \"Output\",\n",
    "            binding,\n",
    "            engine.get_binding_shape(binding),\n",
    "            engine.get_binding_dtype(binding)))\n",
    "\n",
    "    \n",
    "    def binding_nbytes(binding):\n",
    "        return trt.volume(engine.get_binding_shape(binding)) * engine.get_binding_dtype(binding).itemsize\n",
    "    \n",
    "    # Allocate device memory for inputs and outputs.\n",
    "    d_inputs = [cuda.mem_alloc(binding_nbytes(binding)) for binding in engine if engine.binding_is_input(binding)]\n",
    "    h_output = cuda.pagelocked_empty(tuple(engine.get_binding_shape(3)), dtype=np.float32)\n",
    "    d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "\n",
    "    # Create a stream in which to copy inputs/outputs and run inference.\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    print(\"\\nRunning Inference...\")\n",
    "    eval_start_time = time.time()\n",
    "\n",
    "    # Copy inputs\n",
    "    cuda.memcpy_htod_async(d_inputs[0], input_features[\"input_ids\"], stream)\n",
    "    cuda.memcpy_htod_async(d_inputs[1], input_features[\"segment_ids\"], stream)\n",
    "    cuda.memcpy_htod_async(d_inputs[2], input_features[\"input_mask\"], stream)\n",
    "\n",
    "    # Run inference\n",
    "    context.execute_async(bindings=[int(d_inp) for d_inp in d_inputs] + [int(d_output)], stream_handle=stream.handle)\n",
    "    # Transfer predictions back from GPU\n",
    "    cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "\n",
    "    eval_time_elapsed = time.time() - eval_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the inference results let's extract the actual answer to our question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = h_output[:, 0]\n",
    "end_logits = h_output[:, 1]\n",
    "\n",
    "# The total number of n-best predictions to generate in the nbest_predictions.json output file\n",
    "n_best_size = 20\n",
    "\n",
    "# The maximum length of an answer that can be generated. This is needed \n",
    "#  because the start and end predictions are not conditioned on one another\n",
    "max_answer_length = 30\n",
    "\n",
    "\n",
    "(prediction, nbest_json, scores_diff_json) = \\\n",
    "        dp.get_predictions(doc_tokens, features, \\\n",
    "                       start_logits, end_logits, n_best_size, max_answer_length)\n",
    "\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print(\"Running Inference in {:.3f} Sentences/Sec\".format(1.0/eval_time_elapsed))\n",
    "print(\"-----------------------------\")\n",
    "    \n",
    "print(\"Answer: '{}'\".format(prediction))\n",
    "print(\"with prob: {:.3f}%\".format(nbest_json[0]['probability']*100.0))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
